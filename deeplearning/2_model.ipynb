{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files loaded.\n",
      "x_shuffled size: 108\n",
      "y_shuffled size: 108\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed datasets\n",
    "with open('preprocess_x_2.pickle', 'rb') as handle:\n",
    "    x_shuffled = pickle.load(handle)\n",
    "\n",
    "with open('preprocess_y_2.pickle', 'rb') as handle:\n",
    "    y_shuffled = pickle.load(handle)\n",
    "\n",
    "print (\"Files loaded.\")\n",
    "print (\"x_shuffled size: {:d}\".format(len(x_shuffled)))\n",
    "print (\"y_shuffled size: {:d}\".format(len(y_shuffled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 47  67  10  73  15   0  83  80  11  91  36  34  18  12 100  37  35  48\n",
      "  33  65  20  81  72  68  23  45 103  88  68  84  45  84  18  53   7  55\n",
      "  98  74  95  30 103  62  96  96  28  64  80  67  82  10  15   7  31  92\n",
      "  30  55  39  44  89  65   9  22  49  76  79  78   5  53   9  81  97  16\n",
      "  86 107  69  70  42   4  69  99  25  70  79  38  60  26  40  56  17  63\n",
      "  59  39  24 101  90  66  16] TEST: [43 27 66 73 19 90 19 57 54 29 88]\n",
      "TRAIN: [ 74  17  26  19  87   2  52  55  74  10  30  34  77   6  51  48   0  86\n",
      "  44  70  61  94  56  39  78  13  65  37   9  55  11  43 105  13   5  40\n",
      "  83  69  45 101  89  14  99  58  70  85  35   4  81   7  50   1  67   2\n",
      "  21  93  58  15   6  19  99  42 104  41  14  79  73  12   7  98 106  82\n",
      "  71  95  37  24  80  27  10  20  36  93  90 100  49  40  72  92 102  84\n",
      "  72  76  67  33  81  59  64] TEST: [23 76 49 71 94 25 44 69 83 12  3]\n",
      "TRAIN: [ 54  54  86  37  31  26  61  57 105 105  85  48  20  87  99  90  83   8\n",
      "  60  58  86  73 101  41  27  35  40  21  18  19   9 106  77  69   6  17\n",
      "  65  72  45  82  43  91  61  92   1 106  67   6  24  80  21  87  43  23\n",
      "  55  25  67  29  68 101  24  44  39  75   3  51  88 107  82  66   7  14\n",
      "  42  98   0  58  36  22  70  68  97  10 104  39  34  36 103  33  64  89\n",
      "  81  41  32  97  80  38  70] TEST: [107  90  30   4  93  76  50  15  94   5  34]\n",
      "TRAIN: [ 65  91  46   2  84  31   6  96 102  42  31  32  33 104  99  63  66  65\n",
      "  13  70  81  87  91  27  20  26  95  64  16  92  80  50  51  47  30  17\n",
      "  95   2  60  72 101  98  57  99  83  78  96  90  28  29  60  47   3  23\n",
      "  24  55 106  87  15  88  67  41   6  21  48  49  58  38   7  85  79  84\n",
      "  54  82  44  80  52  57  11  40  15  86  19  27   1  35  88  43   5  18\n",
      "  32  69  56  45 105 100  82] TEST: [ 34   9   7  55  38  97  86  76  53 103  51]\n",
      "TRAIN: [ 35  47  29  59   5  78 104  49  63  62 103  99  43  15  73  51  48  57\n",
      " 100  26  90  19  55  50  91   2  91  60  88  80  65  83  13  67  90  42\n",
      "  51  74  87  94  97  54  65  87  69  40  37 102  81  18  77  98  93 107\n",
      "  31  73  49  66   0  56  79  98  20   6  83  66  45  86   0  92  61  62\n",
      "  93  19   4  52  14  12   3   8  27  64  75  60  27  67  10  22  53  68\n",
      "  92  76   7   7  57  18  95] TEST: [33 34 76 46 29  4 21 32 23 55  1]\n",
      "TRAIN: [ 96  67  39  64   8  60  94  24   5  47  28  33   4  86  42  32 105   3\n",
      "  80  75  27  48  57  90  39   5  32  89  15  69  41  89  66  10  77  53\n",
      "  95  37   2  98  55  15  44  43  18 101  23  72 106  21  81   3  56  33\n",
      "  36  48  40 102  54  61  38   4   6  42  80  22  35  50  14 107  88  40\n",
      "  20  75  58  51   7  56  97 107  53  91   9  66   0 102  82  13  50  97\n",
      "  94   0  78 104  25  87  18] TEST: [51 54 73 61 65 35 44 83 74  6 31]\n",
      "TRAIN: [ 86  29  93  31   6  96  23  80  55  42  45  38  57  51  73   4  56  50\n",
      "  23  54  92  24  41  82   2  21  35   6  36  60  66  26  44  40  99  85\n",
      "  47  70  38  52  54  58  16  33  24  61  41  28  71  17  58   5   2  51\n",
      "  53  27  74  50 100  95  81  80  15  19  97  20  96  59  76  32  37  94\n",
      "  11  78 100  98  17  85  11  81  39  15  29  21  33  46  90  75  49   3\n",
      "  26  72  62  89  12  86  18] TEST: [106  25   8  57   9  28   8   1 104  83  99]\n",
      "TRAIN: [ 75  47  96  45  38  21 106  39  56  87  74  72  92  77  37   3  36  42\n",
      "  23  33  41   6  58 103  91  94  60  24  41   4 101  86  10   0  61   8\n",
      "   3 104  47  57  64   5  10  25  89  80   8  78  38  50  37  46   0  84\n",
      "  23  58  91  43 105  86 103  45  49  15  17  32  82  70  26  13  16  28\n",
      "  18  76  48  54  51  52  62  55  53  69  25  14  36  81  95  75  97  18\n",
      "  33 106  99  68  56   9  74] TEST: [98 39 22 60 50 52 42 62 55 92 76]\n",
      "TRAIN: [ 48  67  60  96  43  39  34 106  58  49  66  83 105  28  25   5  64  48\n",
      "  21  79  22  57  32  28  60  37  30  56  75 103  33  65  64  14  69   2\n",
      "  47   4   0  92  95  44   8   3  10   1  41  32  36  51  59   7   0  13\n",
      "  80  62  38  96  47   6  18  42  68  74   5  34  27 101  31  99 104  26\n",
      "  76  63   4  86  91  41  99  72  19  46  87 103  27  70  23 102  78  70\n",
      " 107  49  87  55  68  24  61] TEST: [18 42 17 29 50 94 92 30 53 78 81]\n",
      "TRAIN: [104  87  66  99  91  96  90  94  88  11  60  45  37  28  77   3  47  14\n",
      "  71  54   6  39  55  42  85   8  66  13  30  28  12  71  33  36  96  99\n",
      "  41  30 106  17  29  10   7 107  54  49  86  22 100  23  83  81  90 101\n",
      "  35  61  44  78  79   9  68  44   1  80  80  72  52  64  91  87 105 102\n",
      "  36  48  93  53  38  63  79  89  77  81  74  86  74  23  46 100  94  52\n",
      "  82 104  53  69   0  35  18] TEST: [ 41  21 102   5  70  76  16  82  92  51  34]\n",
      "Train features dimensions: 97, 8405\n",
      "Train labels dimensions: 97, 2\n",
      "Test features dimensions: 11, 8405\n",
      "Test labels dimensions: 11, 2\n"
     ]
    }
   ],
   "source": [
    "# Split train/test set\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=42)\n",
    "for train_ind, test_ind in sss.split(x_shuffled, y_shuffled):\n",
    "    print (\"TRAIN:\", train_ind, \"TEST:\", test_ind)\n",
    "    X_train, X_val = x_shuffled[train_ind], x_shuffled[test_ind]\n",
    "    y_train, y_val = y_shuffled[train_ind], y_shuffled[test_ind]\n",
    "\n",
    "print(\"Train features dimensions: {:d}, {:d}\".format(*X_train.shape))\n",
    "print(\"Train labels dimensions: {:d}, {:d}\".format(*y_train.shape))\n",
    "print(\"Test features dimensions: {:d}, {:d}\".format(*X_val.shape))\n",
    "print(\"Test labels dimensions: {:d}, {:d}\".format(*y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m0.56784\u001b[0m\u001b[0m | time: 1.757s\n",
      "| Adam | epoch: 005 | loss: 0.56784 - acc: 0.8582 -- iter: 96/97\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.55129\u001b[0m\u001b[0m | time: 2.790s\n",
      "| Adam | epoch: 005 | loss: 0.55129 - acc: 0.9038 | val_loss: 0.56559 - val_acc: 0.8182 -- iter: 97/97\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "'''# Building convolutional network\n",
    "network = input_data(shape=[None, 407], name='input')\n",
    "# Converts all words in vocabulary to lower dimensional representation\n",
    "network = tflearn.embedding(network, input_dim=3800, output_dim=128)\n",
    "branch1 = conv_1d(network, 128, 5, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "branch2 = conv_1d(network, 128, 6, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "branch3 = conv_1d(network, 128, 7, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "# Change the shape by adding 2 to dimensions\n",
    "network = tf.expand_dims(network, 2)\n",
    "network = global_max_pool(network)\n",
    "network = dropout(network, 0.5)\n",
    "network = fully_connected(network, 2, activation='softmax')\n",
    "network = regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                     loss='categorical_crossentropy', name='target')\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "model.fit(X_train, y_train, \n",
    "          n_epoch = 5, shuffle=True, \n",
    "          validation_set=(X_val, y_val), \n",
    "          show_metric=True, batch_size=32, \n",
    "          run_id='oc_1')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build TensorFlow model\n",
    "sequence_length = X_train.shape[1]\n",
    "num_classes = y_train.shape[1]\n",
    "vocab_size = 15299\n",
    "embedding_size = 128\n",
    "filter_sizes = [5, 6, 7]\n",
    "num_filters = 128\n",
    "l2_reg_lambda = 0.0\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Placeholders for input, output and dropout\n",
    "    input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "    input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "    # Keep track of L2 regularization loss\n",
    "    l2_loss = tf.constant(0.0)\n",
    "\n",
    "    # Build model\n",
    "    # Embedding layer\n",
    "    with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "        W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"W\")\n",
    "        embedded_chars = tf.nn.embedding_lookup(W, input_x)\n",
    "        embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "\n",
    "    # Create a convolution + maxpool layer for each filter size\n",
    "    pooled_outputs = []\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "            # Convolution layer\n",
    "            filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W-%s\" % filter_size)\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b-%s\" % filter_size)\n",
    "\n",
    "            conv = tf.nn.conv2d(\n",
    "                embedded_chars_expanded,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "\n",
    "            # Maxpooling over the outputs\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"pool\")\n",
    "            pooled_outputs.append(pooled)\n",
    "\n",
    "    # Combine all pooled features\n",
    "    num_filters_total = num_filters * len(filter_sizes)\n",
    "    h_pool = tf.concat(3, pooled_outputs)\n",
    "    h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "\n",
    "    # Add dropout\n",
    "    with tf.name_scope(\"dropout\"):\n",
    "        h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n",
    "\n",
    "    # Final (unnormalized) scores and predictions\n",
    "    with tf.name_scope(\"output\"):\n",
    "        W = tf.get_variable(\n",
    "            \"W\",\n",
    "            shape=[num_filters_total, num_classes],\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "        l2_loss += tf.nn.l2_loss(W)\n",
    "        l2_loss += tf.nn.l2_loss(b)\n",
    "        scores = tf.nn.xw_plus_b(h_drop, W, b, name=\"scores\")\n",
    "        predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "\n",
    "    # Calculate mean cross-entropy loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        losses = tf.nn.softmax_cross_entropy_with_logits(scores, input_y)\n",
    "        loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name=\"accuracy\")\n",
    "        \n",
    "    # AUC\n",
    "    with tf.name_scope(\"auc\"):\n",
    "        a = tf.cast(tf.argmax(predictions, 1),tf.float32)\n",
    "        b = tf.cast(tf.argmax(input_y, 1),tf.float32)\n",
    "        auc = tf.contrib.metrics.streaming_auc(a, b)\n",
    "\n",
    "    # Optimizer\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    starter_learning_rate = 0.0005\n",
    "    # Decay factor of 0.95 after every 10000 steps.\n",
    "    with tf.name_scope('learning_rate'):\n",
    "        learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 10000, 0.95)\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 10: 0.478298\n",
      "Minibatch accuracy: 0.9091\n",
      "Validation accuracy: 0.9091\n",
      "Saved model checkpoint to /home/ubuntu/pynb/oscars/cp-10\n",
      "\n",
      "Minibatch loss at step 20: 0.297160\n",
      "Minibatch accuracy: 0.9697\n",
      "Validation accuracy: 0.9091\n",
      "Saved model checkpoint to /home/ubuntu/pynb/oscars/cp-20\n",
      "\n",
      "Minibatch loss at step 30: 0.134719\n",
      "Minibatch accuracy: 0.9697\n",
      "Validation accuracy: 0.9091\n",
      "Saved model checkpoint to /home/ubuntu/pynb/oscars/cp-30\n",
      "\n",
      "Minibatch loss at step 40: 0.010866\n",
      "Minibatch accuracy: 1.0000\n",
      "Validation accuracy: 0.9091\n",
      "Saved model checkpoint to /home/ubuntu/pynb/oscars/cp-40\n",
      "\n",
      "Minibatch loss at step 50: 0.119697\n",
      "Minibatch accuracy: 0.9697\n",
      "Validation accuracy: 0.9091\n",
      "Saved model checkpoint to /home/ubuntu/pynb/oscars/cp-50\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3576a7942f44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mevaluate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0maccuracy_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-3576a7942f44>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         _, step, l, accuracy_train = sess.run(\n\u001b[0;32m---> 17\u001b[0;31m             [optimizer, global_step, loss, accuracy], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ======== Training =========\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #tf.initialize_all_variables().run()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    sess.run(tf.initialize_local_variables())\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    print('Initialized')\n",
    "\n",
    "    def train_step(x_batch, y_batch):\n",
    "        feed_dict = {\n",
    "                input_x: x_batch,\n",
    "                input_y: y_batch,\n",
    "                dropout_keep_prob: 0.5\n",
    "                }\n",
    "        \n",
    "        _, step, l, accuracy_train = sess.run(\n",
    "            [optimizer, global_step, loss, accuracy], feed_dict=feed_dict)\n",
    "        return step, l, accuracy_train\n",
    "        \n",
    "    def val_step(x_val, y_val):\n",
    "        feed_dict = {\n",
    "                input_x: x_val,\n",
    "                input_y: y_val,\n",
    "                dropout_keep_prob: 1.0\n",
    "                }\n",
    "            \n",
    "        step, loss_val, accuracy_val = sess.run(\n",
    "            [global_step, loss, accuracy], feed_dict=feed_dict)\n",
    "        return accuracy_val\n",
    "    \n",
    "    def batch_iter(data, batch_size, num_epochs, shuffle=False):\n",
    "        '''\n",
    "        Generates a batch iterator for a dataset.\n",
    "        '''\n",
    "        data = np.array(data)\n",
    "        data_size = len(data)\n",
    "        num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                yield shuffled_data[start_index:end_index]\n",
    "\n",
    "    batch_size = 64\n",
    "    num_epochs = 50\n",
    "    evaluate_every = 10\n",
    "    checkpoint_every = 10\n",
    "    checkpoint= '/home/ubuntu/pynb/oscars/cp'\n",
    "\n",
    "    # Generate batches\n",
    "    batches = batch_iter(\n",
    "        list(zip(X_train, y_train)), batch_size, num_epochs)\n",
    "    # Training loop. For each batch...\n",
    "    for batch in batches:\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        step, l, accuracy_train = train_step(x_batch, y_batch)\n",
    "        if (step % evaluate_every == 0):\n",
    "            accuracy_val = val_step(X_val, y_val)\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: {:.4f}'.format(accuracy_train))\n",
    "            print('Validation accuracy: {:.4f}'.format(accuracy_val))\n",
    "            #print('Minibatch AUC: {:.4f}'.format(auc_train))\n",
    "            #print('Validation AUC: {:.4f}'.format(auc_val))\n",
    "        if (step % checkpoint_every == 0):\n",
    "            path = saver.save(sess, checkpoint, global_step=step)\n",
    "            print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37255746126174927, 0.6274425387382507], [0.37758776545524597, 0.6224122047424316], [0.3729100525379181, 0.6270899772644043], [0.37365567684173584, 0.6263443231582642], [0.38283562660217285, 0.6171643733978271]]\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed test dataset\n",
    "'''with open('preprocess_test_1.pickle', 'rb') as handle:\n",
    "    X_test = pickle.load(handle)\n",
    "    \n",
    "pred = model.predict(X_test)\n",
    "print (pred)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
